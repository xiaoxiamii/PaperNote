改变损失函数对模型和训练结果的影响
===================

问题的提出
-------------

主要问题在于如果对同一个模型，更换损失函数的意义是什么，在做天池比赛的时候用过GBDT和GBRT，二者模型的架构是一样的，主要的区别还是模型的损失函数不一样，[GBDT](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier)采用的是deviance-残差。 [GBRT](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor)采用的是LS- 最小二乘法。本文主要讨论在同一个模型下，什么时候会采用不同的损失函数，以及**改变损失函数对模型和结果的影响**。


----------


处理不平衡的数据时候，改变损失函数
-------------

假设我们遇到一个二分类问题，当正样本有500个，而负样本远远多于正样本，是正样本的10000倍，500W个，处理这个问题的时候，可以采用改变**损失函数**

> **可行方法:**

> - 定义损失函数的时候，把正样本的损失函数定义的比负样本大，比如定义为负样本的L倍。
> - 用SGD进行求解的时候,在遇到正样本的时候迭代L次。
> - 将负样本分成L堆,每次取其中的一堆负样本和整个正样本训练一个分类器,一共训练L个分类器.最后用模型组合的方法把这L个分类器组合起来。
> 采用One Class Classifier,把正样本当作是outlier。
> 采用SMOTE或者SMOTEBoost方法合成更多的数据。
> 方法5和方法3可以结合起来用。

[参考链接](http://leijun00.github.io/2015/06/unbalanced-dataset/)


----------

根据数据集决定损失函数
-------------------

以下关于模型的讨论主要基于LR中的损失函数采用log似然-交叉熵 还是 hingeloss 方差

**满足各自的响应变量y服从不同的概率分布**

对于一个数据集，首先你要根据数据的特点和目的来选择合适模型。
就你问的而言，选定的模型是Logistic Regression。现在既然已经选择了模型，那么接下来的问题是：怎么才能让这个模型尽可能好的拟合或者分类数据呢？那么就需要有目标，所以要定下模型的cost function，但是cost function怎么定呢？凭直觉随便选吗！不！可！能！
我们都知道，Linear Regression的cost function是最小二乘，即

但是Logistic Regression的cost function却是
为什么Logistic Regression不使用最小二乘做cost function呢？

答案是各自的响应变量服从不同的概率分布。
在Linear Regression中，前提假设是服从正态分布，即，而Logistic中的是服从二项分布的，即。(为什么不服从正态？因为非0即1啊！)
因而，在用极大似然估计计算时，所得到的cost function自然是不一样的。(可自行推导)

然而，只有目标是没用的，我们还要有方法来达到目标，这里的方法就是上述的算法——最优化算法。包括常用的梯度下降法(最速下降法)、牛顿法、拟牛顿法等。这样，一个机器学习算法就算完整了，因为可以用这些最优化算法来求出。

所以！结论是：三者完全没有可比性！
由一些前提假设和极大似然估计从概率的角度推导出了cost function（Linear中是最小二乘，Logistic中是对数似然），而梯度下降只是一个最优化算法，用来优化cost function的

链接：http://www.zhihu.com/question/24900876/answer/65176508

----------

根据业务需求决定
-------------

假如在业务中对精确度要求很高，召回度要求低，
或者精确度要求低，召回度要求高，
可以采用不同的损失函数来
目前没有找到 什么损失函数的精确度高，召回低，或者反例，感觉应该是根据业务场景决定的。


对参数优化有影响
-------------

梯度下降又称最速下降法，目标函数一阶泰勒展开近似得到的，最小二乘法我觉得在线性条件下可以看作牛顿法，利用了二阶导数信息，按理说收敛速度应该更快一些。我觉得一个原因在于二阶导数实在不好求，即使后续发展了拟牛顿、BFGS这些近似方法，也肯定不如梯度下降法简单有效。另外，注意逻辑回归里loss function大多采用交叉熵形式，而非均方误差的形式，最小二乘未必通用。这里有解释为什么常采用交叉熵形式的损失函数[Neural networks and deep learning](http://link.zhihu.com/?target=http://neuralnetworksanddeeplearning.com/chap3.html)，主要结论是**交叉熵的收敛速度恰好为预测值与实际值的误差，这样也就意味着误差越大收敛越快，因此其初始点的选取对结果影响不是特别大，而均方误差形式的初始点选取对误差曲线影响很大**。

PS 个人理解，对于均方误差初始点的选取对误差曲线

链接：http://www.zhihu.com/question/24900876/answer/46616933
参考链接[Neural networks and deep learning](http://link.zhihu.com/?target=http://neuralnetworksanddeeplearning.com/chap3.html)

----------
为什么用最小二乘（求残差平方和最小值）来拟合函数
-------------

线性回归 是以 高斯分布 为误差分析模型； 逻辑回归 采用的是 伯努利分布 分析误差

为什么用最小二乘（求残差平方和最小值）来拟合函数？

残差可以理解，那为什么是平方和，而不是绝对值或者四次方和？对此，极大似然可以完美解释。
为大量独立样本线性拟合，根据中心极限定理可知，其残差呈高斯分布。也就是说，X确定时，y的可能性呈高斯分布。这时，对于给定的参数θ，我们可以求出P(y|X;θ)，表示使用这组θ的情况下，给定一组X，得出这组y的可能性。而关于θ的似然函数可以写为L(θ)=P(y|X;θ)，表示已知(X,y)的情况下，θ值出现的可能性。明显的，当L(θ)最大时，θ为最佳拟合参数。求L(θ)最大值时，我们会在结果中发现残差平方和的式子，并且残差平方和最小L(θ)最大。
梯度下降法，是先给定一组系数θ，根据最小二乘法构造一个损失函数J(θ)。然后不断迭代以找出新的θ来使J(θ)更小。最终确定最合适的θ。

链接：http://www.zhihu.com/question/24900876/answer/52906650
